---
date: 2021-06-02
Source:
tags:
  - mathematics 
aliases: 
  - Shannon Entropy
---
Given a random variable $X$ with probability distribution $P$, its entropy is given by
$$
H(X) \equiv -\sumlim_x P(x) \log P(x)
$$

Due to the [[Mutual Information]] definition, one can say that the entropy is a measure of **self-information** of $X$.

---
### References
- 